@article{yu2025llm_decision_sim,
  title = {A Large-Scale Simulation on Large Language Models for Decision-Making},
  author = {Chenxiao Yu* and Jinyi Ye and Lincan Li and Zhaoyang Li and 
            Emilio Ferrara and Xiyang Hu and Yue Zhao},
  journal = {Nature Communications},
  year = {2025},
  note = {Under review},
  url = {https://arxiv.org/abs/2412.15291},
  selected = {true},
  description = {An extensive simulation study analyzing decision-making patterns of LLMs across identity conditions, threat cues, and moral scenarios.},
  keywords = {LLM Simulation, Decision-Making, Generative Agents, Social Reasoning},
  abstract = {We conduct a large-scale simulation to analyze how LLMs make decisions under varying identity cues, threat conditions, and moral contexts. Our results show that LLM decisions are shaped by internal representations of group identity and perceived threat, revealing systematic patterns that resemble human social behavior. The study provides insights into the stability and alignment challenges of generative-agent simulations.}
}

@article{yu2025stealthrank,
  title = {StealthRank: LLM Ranking Manipulation via Stealthy Prompt Optimization},
  author = {Yiming Tang* and Yi Fan and Chenxiao Yu* and Tiankai Yang and 
            Yue Zhao and Xiyang Hu},
  journal = {Transactions on Machine Learning Research},
  year = {2025},
  note = {Submitted},
  url = {https://arxiv.org/abs/2504.05804},
  selected = {true},
  description = {A framework that subtly manipulates LLM ranking outputs using optimized stealth prompts while evading detection.},
  keywords = {LLM Ranking, Prompt Optimization, Adversarial Attacks, Stealth Manipulation},
  abstract = {We study prompt-based ranking manipulation in LLMs and introduce StealthRank, an optimization framework that generates subtle adversarial prompts capable of shifting model rankings while remaining difficult to detect. Our experiments show consistent manipulation effects across instruction-tuned models without triggering standard defense mechanisms.}
}

@inproceedings{yu2026causal_llm,
  title = {Mitigating Hallucinations in Large Language Models via Causal Reasoning},
  author = {Yuangang Li and Yiqing Shen and Yi Nian and Jiechao Gao and 
            Ziyi Wang and Chenxiao Yu and Shawn Li and Jie Wang and 
            Xiyang Hu and Yue Zhao},
  year = {2026},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  url = {https://arxiv.org/abs/2508.12495},
  selected = {true},
  description = {A causal intervention framework that reduces hallucinations in LLMs by identifying and adjusting unstable internal pathways.},
  keywords = {LLM Safety, Causality, Hallucination Mitigation, Model Reliability},
  abstract = {This work introduces a causal reasoning framework for reducing hallucinations in large language models. By analyzing activation-level dependencies and intervening on unstable causal edges, we develop mitigation strategies that improve answer consistency across diverse tasks. Experiments on multiple LLM architectures show reduced hallucination rates in both open-ended and factual question settings.}
}

@article{yu2024political_llm,
  title = {Political-LLM: Large Language Models in Political Science},
  author = {Lincan Li and Jiaqi Li and Catherine Chen and Fred Gui and Hongjia Yang and 
            Chenxiao Yu* and Zhengguang Wang and Jianing Cai and 
            Junlong Aaron Zhou and Bolin Shen and others},
  journal = {Transactions on Machine Learning Research},
  year = {2024},
  note = {Under review},
  url = {https://arxiv.org/abs/2412.06864},
  description = {A large-scale evaluation of political reasoning, ideology prediction, and bias in modern LLMs using political science benchmarks.},
  keywords = {Political Science, LLM Evaluation, Ideology Modeling, Social Reasoning},
  abstract = {This paper presents Political-LLM, a comprehensive benchmark suite and evaluation framework for analyzing the political reasoning capabilities of large language models. We examine ideological bias, political knowledge, persuasion dynamics, and group-based reasoning across multiple model families. The results uncover systematic biases and reveal divergence between LLM predictions and human survey distributions.}
}

