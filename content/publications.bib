@article{li2026defenses,
  title = {Defenses Against Prompt Attacks Learn Surface Heuristics},
  author = {Shawn Li* and Chenxiao Yu* and Zhiyu Ni* and Hao Li and 
            Charith Peris and Chaowei Xiao and Yue Zhao},
  journal = {arXiv preprint arXiv:2601.07185},
  year = {2026},
  url = {https://arxiv.org/abs/2601.07185},
  selected = {true},
  description = {Current defenses against prompt injection attacks rely on surface-level heuristics rather than robust understanding of attack patterns.},
  keywords = {Prompt Security, LLM Safety, Adversarial Attacks, Defense Mechanisms}
}

@misc{yu2026tracingmoralfoundations,
  title = {Tracing Moral Foundations in Large Language Models}, 
  author = {Chenxiao Yu* and Bowen Yi and Farzan Karimi-Malekabadi and 
            Suhaib Abdurahman and Jinyi Ye and Shrikanth Narayanan and 
            Yue Zhao and Morteza Dehghani},
  year = {2026},
  eprint = {2601.05437},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  url = {https://arxiv.org/abs/2601.05437},
  selected = {true},
  description = {Do LLMs have human-like moral cognition?},
  keywords = {Moral Foundations Theory, LLM Interpretability, Computational Psychology, Cognition Stability}
}

@misc{abdurahman2025realistic,
  title = {Realistic threat perception drives intergroup conflict: A causal, dynamic analysis using generative-agent simulations}, 
  author = {Suhaib Abdurahman and Farzan Karimi-Malekabadi and Chenxiao Yu and 
            Nour S. Kteily and Morteza Dehghani},
  year = {2026},
  eprint = {2512.17066},
  archivePrefix = {arXiv},
  primaryClass = {cs.AI},
  url = {https://arxiv.org/abs/2512.17066},
  selected = {true},
  description = {A causal analysis of how realistic threat perception drives intergroup conflict using generative-agent simulations.},
  keywords = {Generative Agents, Intergroup Conflict, Threat Perception, Social Simulation}
}

@article{yu2025llm_decision_sim,
  title = {A Large-Scale Simulation on Large Language Models for Decision-Making},
  author = {Chenxiao Yu* and Jinyi Ye and Lincan Li and Zhaoyang Li and 
            Emilio Ferrara and Xiyang Hu and Yue Zhao},
  year = {2025},
  note = {Under review},
  url = {https://arxiv.org/abs/2412.15291},
  selected = {true},
  description = {An extensive simulation study analyzing decision-making patterns of LLMs across identity conditions, threat cues, and moral scenarios.},
  keywords = {LLM Simulation, Decision-Making, Generative Agents, Social Reasoning},
  abstract = {We conduct a large-scale simulation to analyze how LLMs make decisions under varying identity cues, threat conditions, and moral contexts. Our results show that LLM decisions are shaped by internal representations of group identity and perceived threat, revealing systematic patterns that resemble human social behavior. The study provides insights into the stability and alignment challenges of generative-agent simulations.}
}

@article{yu2025stealthrank,
  title = {StealthRank: LLM Ranking Manipulation via Stealthy Prompt Optimization},
  author = {Yiming Tang* and Yi Fan and Chenxiao Yu* and Tiankai Yang and 
            Yue Zhao and Xiyang Hu},
  year = {2025},
  note = {Submitted},
  url = {https://arxiv.org/abs/2504.05804},
  selected = {true},
  description = {A framework that subtly manipulates LLM ranking outputs using optimized stealth prompts while evading detection.},
  keywords = {LLM Ranking, Prompt Optimization, Adversarial Attacks, Stealth Manipulation},
  abstract = {We study prompt-based ranking manipulation in LLMs and introduce StealthRank, an optimization framework that generates subtle adversarial prompts capable of shifting model rankings while remaining difficult to detect. Our experiments show consistent manipulation effects across instruction-tuned models without triggering standard defense mechanisms.}
}

@inproceedings{yu2026causal_llm,
  title = {Mitigating Hallucinations in Large Language Models via Causal Reasoning},
  author = {Yuangang Li and Yiqing Shen and Yi Nian and Jiechao Gao and 
            Ziyi Wang and Chenxiao Yu and Shawn Li and Jie Wang and 
            Xiyang Hu and Yue Zhao},
  year = {2026},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  url = {https://arxiv.org/abs/2508.12495},
  selected = {true},
  description = {A causal intervention framework that reduces hallucinations in LLMs by identifying and adjusting unstable internal pathways.},
  keywords = {LLM Safety, Causality, Hallucination Mitigation, Model Reliability},
  abstract = {This work introduces a causal reasoning framework for reducing hallucinations in large language models. By analyzing activation-level dependencies and intervening on unstable causal edges, we develop mitigation strategies that improve answer consistency across diverse tasks. Experiments on multiple LLM architectures show reduced hallucination rates in both open-ended and factual question settings.}
}

@article{yu2024political_llm,
  title = {Political-LLM: Large Language Models in Political Science},
  author = {Lincan Li and Jiaqi Li and Catherine Chen and Fred Gui and Hongjia Yang and 
            Chenxiao Yu* and Zhengguang Wang and Jianing Cai and 
            Junlong Aaron Zhou and Bolin Shen and others},
  journal = {Transactions on Machine Learning Research},
  year = {2024},
  note = {Under review},
  url = {https://arxiv.org/abs/2412.06864},
  description = {A large-scale evaluation of political reasoning, ideology prediction, and bias in modern LLMs using political science benchmarks.},
  keywords = {Political Science, LLM Evaluation, Ideology Modeling, Social Reasoning},
  abstract = {This paper presents Political-LLM, a comprehensive benchmark suite and evaluation framework for analyzing the political reasoning capabilities of large language models. We examine ideological bias, political knowledge, persuasion dynamics, and group-based reasoning across multiple model families. The results uncover systematic biases and reveal divergence between LLM predictions and human survey distributions.}
}

